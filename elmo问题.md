说一说elmo缺点。

ELMO之后有了Bert，这样回看，ELMO 有什么值得改进的缺点呢？
首先，一个非常明显的缺点在特征抽取器选择方面，ELMO 使用了 LSTM 而不是新贵 Transformer，
Transformer 是谷歌在 17 年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响，
很多研究已经证明了 Transformer 提取特征的能力是要远强于 LSTM 的。如果 ELMO 采取 Transformer 作为特征提取器，
那么估计 Bert 的反响远不如现在的这种火爆场面。
另外一点，ELMO 采取双向拼接这种融合特征的能力可能比 Bert 一体化的融合特征方式弱，
但是，这只是一种从道理推断产生的怀疑，目前并没有具体实验说明这一点。

训练的过程感觉很慢。因为对每个token编码都要通过language model计算的出, 
不如之前fix的embedding直接拿来用, 效率低到令人发指, 
没有充足的计算资源会很难受. 这里一个解决办法是, 
我们一般对模型需要多轮的训练, 每次训练都会重新通过language model计算token, 
而我们不进行梯度回传更新biLM的参数, 所以我们输入相同的句子(文章或其他序列)输出结果不会改变, 
因此我们可以只在第一个epoch中通过biLM计算token的表示, 
然后我们保存起来, 下一次用到这个序列时直接加载, 
可以节省大量时间, 这方面的分析见下一小节.
