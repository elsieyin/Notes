说一说elmo缺点。

ELMO之后有了Bert，这样回看，ELMO 有什么值得改进的缺点呢？
首先，一个非常明显的缺点在特征抽取器选择方面，ELMO 使用了 LSTM 而不是新贵 Transformer，
Transformer 是谷歌在 17 年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响，
很多研究已经证明了 Transformer 提取特征的能力是要远强于 LSTM 的。如果 ELMO 采取 Transformer 作为特征提取器，
那么估计 Bert 的反响远不如现在的这种火爆场面。
另外一点，ELMO 采取双向拼接这种融合特征的能力可能比 Bert 一体化的融合特征方式弱，
但是，这只是一种从道理推断产生的怀疑，目前并没有具体实验说明这一点。

训练的过程感觉很慢。因为对每个token编码都要通过language model计算的出, 
不如之前fix的embedding直接拿来用, 效率低到令人发指, 
没有充足的计算资源会很难受. 这里一个解决办法是, 
我们一般对模型需要多轮的训练, 每次训练都会重新通过language model计算token, 
而我们不进行梯度回传更新biLM的参数, 所以我们输入相同的句子(文章或其他序列)输出结果不会改变, 
因此我们可以只在第一个epoch中通过biLM计算token的表示, 
然后我们保存起来, 下一次用到这个序列时直接加载, 
可以节省大量时间, 这方面的分析见下一小节.
因为我们需要训练很多歌epoch才能让模型收敛, 
而ELMo虽然对同一个单词会编码出不同的结果, 
但是上下文相同的时候ELMo编码出的结果是不变的(这里不进行回传更新LM的参数), 
为了解决上面的问题, 我们可以将数据集中的所有词的ELMo编码存起来(不同epoch共享同一个编码, 
同一个单词编码还是上下文相关的), 这里又有一个新的问题–空间问题, 
上文已经提到ELMo每个词编码成31024维的向量, 
每个用一个单精度float表示, 共需31024*4Byte =12KB, 一个单词的编码就需要12KB数据来表示它的语义信息, 
1GB内存也就能存个8万多个词的编码, 
像上文提到的SQuAD需要约480G的内存来保存所有词的编码信息, 所以这是一个鱼和熊掌的选择.

其实解决方案还是有的, 因为论文中发现不同任务对不同层的LM编码信息的敏感程度不同,
比如SQuAD只对第一和第二层的编码信息敏感, 那我们保存的时候可以只保存ELMo编码的一部分, 
在SQuAD中只保存前两层, 存储空间可以降低1/3, 需要320G就可以了, 
如果我们事先确定数据集对于所有不同层敏感程度(即上文提到的sj), 
我们可以直接用系数超参sj对3层的输出直接用∑Lj=0staskjhLMk,j压缩成一个1024的向量,
这样只需要160G的存储空间即可满足需求.
